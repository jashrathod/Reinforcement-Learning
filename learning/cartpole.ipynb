{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "#                 self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 15.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 34.55 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 156.56 ticks.\n",
      "Ran 236 episodes. Solved after 136 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "    # gym.upload('tmp/cartpole-1', api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "SCORES_CSV_PATH = \"./scores/scores.csv\"\n",
    "SCORES_PNG_PATH = \"./scores/scores.png\"\n",
    "SOLVED_CSV_PATH = \"./scores/solved.csv\"\n",
    "SOLVED_PNG_PATH = \"./scores/solved.png\"\n",
    "AVERAGE_SCORE_TO_SOLVE = 195\n",
    "CONSECUTIVE_RUNS_TO_SOLVE = 100\n",
    "\n",
    "\n",
    "class ScoreLogger:\n",
    "\n",
    "    def __init__(self, env_name):\n",
    "        self.scores = deque(maxlen=CONSECUTIVE_RUNS_TO_SOLVE)\n",
    "        self.env_name = env_name\n",
    "\n",
    "        if os.path.exists(SCORES_PNG_PATH):\n",
    "            os.remove(SCORES_PNG_PATH)\n",
    "        if os.path.exists(SCORES_CSV_PATH):\n",
    "            os.remove(SCORES_CSV_PATH)\n",
    "\n",
    "    def add_score(self, score, run):\n",
    "        self._save_csv(SCORES_CSV_PATH, score)\n",
    "        self._save_png(input_path=SCORES_CSV_PATH,\n",
    "                       output_path=SCORES_PNG_PATH,\n",
    "                       x_label=\"runs\",\n",
    "                       y_label=\"scores\",\n",
    "                       average_of_n_last=CONSECUTIVE_RUNS_TO_SOLVE,\n",
    "                       show_goal=True,\n",
    "                       show_trend=True,\n",
    "                       show_legend=True)\n",
    "        self.scores.append(score)\n",
    "        mean_score = mean(self.scores)\n",
    "        print(\"Scores: (min: \" + str(min(self.scores)) + \", avg: \" + str(mean_score) + \", max: \" + str(max(self.scores)) + \")\\n\")\n",
    "        if mean_score >= AVERAGE_SCORE_TO_SOLVE and len(self.scores) >= CONSECUTIVE_RUNS_TO_SOLVE:\n",
    "            solve_score = run-CONSECUTIVE_RUNS_TO_SOLVE\n",
    "            print(\"Solved in \" + str(solve_score) + \" runs, \" + str(run) + \" total runs.\")\n",
    "            self._save_csv(SOLVED_CSV_PATH, solve_score)\n",
    "            self._save_png(input_path=SOLVED_CSV_PATH,\n",
    "                           output_path=SOLVED_PNG_PATH,\n",
    "                           x_label=\"trials\",\n",
    "                           y_label=\"steps before solve\",\n",
    "                           average_of_n_last=None,\n",
    "                           show_goal=False,\n",
    "                           show_trend=False,\n",
    "                           show_legend=False)\n",
    "            exit()\n",
    "\n",
    "    def _save_png(self, input_path, output_path, x_label, y_label, average_of_n_last, show_goal, show_trend, show_legend):\n",
    "        x = []\n",
    "        y = []\n",
    "        with open(input_path, \"r\") as scores:\n",
    "            reader = csv.reader(scores)\n",
    "            data = list(reader)\n",
    "            for i in range(0, len(data)):\n",
    "                x.append(int(i))\n",
    "                y.append(int(data[i][0]))\n",
    "\n",
    "        plt.subplots()\n",
    "        plt.plot(x, y, label=\"score per run\")\n",
    "\n",
    "        average_range = average_of_n_last if average_of_n_last is not None else len(x)\n",
    "        plt.plot(x[-average_range:], [np.mean(y[-average_range:])] * len(y[-average_range:]), linestyle=\"--\", label=\"last \" + str(average_range) + \" runs average\")\n",
    "\n",
    "        if show_goal:\n",
    "            plt.plot(x, [AVERAGE_SCORE_TO_SOLVE] * len(x), linestyle=\":\", label=str(AVERAGE_SCORE_TO_SOLVE) + \" score average goal\")\n",
    "\n",
    "        if show_trend and len(x) > 1:\n",
    "            trend_x = x[1:]\n",
    "            z = np.polyfit(np.array(trend_x), np.array(y[1:]), 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(trend_x, p(trend_x), linestyle=\"-.\",  label=\"trend\")\n",
    "\n",
    "        plt.title(self.env_name)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "\n",
    "        if show_legend:\n",
    "            plt.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    def _save_csv(self, path, score):\n",
    "        if not os.path.exists(path):\n",
    "            with open(path, \"w\"):\n",
    "                pass\n",
    "        scores_file = open(path, \"a\")\n",
    "        with scores_file:\n",
    "            writer = csv.writer(scores_file)\n",
    "            writer.writerow([score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(64, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(48, activation=\"tanh\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    score_logger = ScoreLogger(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    for abc in range(50):\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "#             env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n",
    "                score_logger.add_score(step, run)\n",
    "                break\n",
    "            dqn_solver.experience_replay()\n",
    "        dqn_solver.model.save('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1220 16:04:37.828201 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1220 16:04:37.844059 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1220 16:04:37.847249 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1220 16:04:37.911696 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1220 16:04:38.181806 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1220 16:04:38.182546 4547562816 deprecation_wrapper.py:119] From /Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (min: 16, avg: 16, max: 16)\n",
      "\n",
      "Run: 2, exploration: 0.8061065909263957, score: 47\n",
      "Scores: (min: 16, avg: 31.5, max: 47)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashrathod/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/ipykernel_launcher.py:77: RankWarning: Polyfit may be poorly conditioned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 3, exploration: 0.736559652908221, score: 19\n",
      "Scores: (min: 16, avg: 27.333333333333332, max: 47)\n",
      "\n",
      "Run: 4, exploration: 0.6696478204705644, score: 20\n",
      "Scores: (min: 16, avg: 25.5, max: 47)\n",
      "\n",
      "Run: 5, exploration: 0.5997278763867329, score: 23\n",
      "Scores: (min: 16, avg: 25, max: 47)\n",
      "\n",
      "Run: 6, exploration: 0.5344229416520513, score: 24\n",
      "Scores: (min: 16, avg: 24.833333333333332, max: 47)\n",
      "\n",
      "Run: 7, exploration: 0.46677573701590436, score: 28\n",
      "Scores: (min: 16, avg: 25.285714285714285, max: 47)\n",
      "\n",
      "Run: 8, exploration: 0.3995984329713264, score: 32\n",
      "Scores: (min: 16, avg: 26.125, max: 47)\n",
      "\n",
      "Run: 9, exploration: 0.3632974174544486, score: 20\n",
      "Scores: (min: 16, avg: 25.444444444444443, max: 47)\n",
      "\n",
      "Run: 10, exploration: 0.2987875242397482, score: 40\n",
      "Scores: (min: 16, avg: 26.9, max: 47)\n",
      "\n",
      "Run: 11, exploration: 0.24820838415550486, score: 38\n",
      "Scores: (min: 16, avg: 27.90909090909091, max: 47)\n",
      "\n",
      "Run: 12, exploration: 0.20931540516921554, score: 35\n",
      "Scores: (min: 16, avg: 28.5, max: 47)\n",
      "\n",
      "Run: 13, exploration: 0.1572956799768517, score: 58\n",
      "Scores: (min: 16, avg: 30.76923076923077, max: 58)\n",
      "\n",
      "Run: 14, exploration: 0.13331482894782642, score: 34\n",
      "Scores: (min: 16, avg: 31, max: 58)\n",
      "\n",
      "Run: 15, exploration: 0.08662902049662846, score: 87\n",
      "Scores: (min: 16, avg: 34.733333333333334, max: 87)\n",
      "\n",
      "Run: 16, exploration: 0.030847362533106718, score: 207\n",
      "Scores: (min: 16, avg: 45.5, max: 207)\n",
      "\n",
      "Run: 17, exploration: 0.0175075817047932, score: 114\n",
      "Scores: (min: 16, avg: 49.529411764705884, max: 207)\n",
      "\n",
      "Run: 18, exploration: 0.01, score: 123\n",
      "Scores: (min: 16, avg: 53.611111111111114, max: 207)\n",
      "\n",
      "Run: 19, exploration: 0.01, score: 125\n",
      "Scores: (min: 16, avg: 57.36842105263158, max: 207)\n",
      "\n",
      "Run: 20, exploration: 0.01, score: 136\n",
      "Scores: (min: 16, avg: 61.3, max: 207)\n",
      "\n",
      "Run: 21, exploration: 0.01, score: 132\n",
      "Scores: (min: 16, avg: 64.66666666666667, max: 207)\n",
      "\n",
      "Run: 22, exploration: 0.01, score: 166\n",
      "Scores: (min: 16, avg: 69.27272727272727, max: 207)\n",
      "\n",
      "Run: 23, exploration: 0.01, score: 240\n",
      "Scores: (min: 16, avg: 76.69565217391305, max: 240)\n",
      "\n",
      "Run: 24, exploration: 0.01, score: 76\n",
      "Scores: (min: 16, avg: 76.66666666666667, max: 240)\n",
      "\n",
      "Run: 25, exploration: 0.01, score: 53\n",
      "Scores: (min: 16, avg: 75.72, max: 240)\n",
      "\n",
      "Run: 26, exploration: 0.01, score: 172\n",
      "Scores: (min: 16, avg: 79.42307692307692, max: 240)\n",
      "\n",
      "Run: 27, exploration: 0.01, score: 124\n",
      "Scores: (min: 16, avg: 81.07407407407408, max: 240)\n",
      "\n",
      "Run: 28, exploration: 0.01, score: 154\n",
      "Scores: (min: 16, avg: 83.67857142857143, max: 240)\n",
      "\n",
      "Run: 29, exploration: 0.01, score: 178\n",
      "Scores: (min: 16, avg: 86.93103448275862, max: 240)\n",
      "\n",
      "Run: 30, exploration: 0.01, score: 154\n",
      "Scores: (min: 16, avg: 89.16666666666667, max: 240)\n",
      "\n",
      "Run: 31, exploration: 0.01, score: 175\n",
      "Scores: (min: 16, avg: 91.93548387096774, max: 240)\n",
      "\n",
      "Run: 32, exploration: 0.01, score: 139\n",
      "Scores: (min: 16, avg: 93.40625, max: 240)\n",
      "\n",
      "Run: 33, exploration: 0.01, score: 126\n",
      "Scores: (min: 16, avg: 94.39393939393939, max: 240)\n",
      "\n",
      "Run: 34, exploration: 0.01, score: 178\n",
      "Scores: (min: 16, avg: 96.8529411764706, max: 240)\n",
      "\n",
      "Run: 35, exploration: 0.01, score: 136\n",
      "Scores: (min: 16, avg: 97.97142857142858, max: 240)\n",
      "\n",
      "Run: 36, exploration: 0.01, score: 134\n",
      "Scores: (min: 16, avg: 98.97222222222223, max: 240)\n",
      "\n",
      "Run: 37, exploration: 0.01, score: 179\n",
      "Scores: (min: 16, avg: 101.13513513513513, max: 240)\n",
      "\n",
      "Run: 38, exploration: 0.01, score: 114\n",
      "Scores: (min: 16, avg: 101.47368421052632, max: 240)\n",
      "\n",
      "Run: 39, exploration: 0.01, score: 138\n",
      "Scores: (min: 16, avg: 102.41025641025641, max: 240)\n",
      "\n",
      "Run: 40, exploration: 0.01, score: 140\n",
      "Scores: (min: 16, avg: 103.35, max: 240)\n",
      "\n",
      "Run: 41, exploration: 0.01, score: 112\n",
      "Scores: (min: 16, avg: 103.5609756097561, max: 240)\n",
      "\n",
      "Run: 42, exploration: 0.01, score: 196\n",
      "Scores: (min: 16, avg: 105.76190476190476, max: 240)\n",
      "\n",
      "Run: 43, exploration: 0.01, score: 31\n",
      "Scores: (min: 16, avg: 104.02325581395348, max: 240)\n",
      "\n",
      "Run: 44, exploration: 0.01, score: 131\n",
      "Scores: (min: 16, avg: 104.63636363636364, max: 240)\n",
      "\n",
      "Run: 45, exploration: 0.01, score: 135\n",
      "Scores: (min: 16, avg: 105.31111111111112, max: 240)\n",
      "\n",
      "Run: 46, exploration: 0.01, score: 87\n",
      "Scores: (min: 16, avg: 104.91304347826087, max: 240)\n",
      "\n",
      "Run: 47, exploration: 0.01, score: 213\n",
      "Scores: (min: 16, avg: 107.2127659574468, max: 240)\n",
      "\n",
      "Run: 48, exploration: 0.01, score: 143\n",
      "Scores: (min: 16, avg: 107.95833333333333, max: 240)\n",
      "\n",
      "Run: 49, exploration: 0.01, score: 144\n",
      "Scores: (min: 16, avg: 108.6938775510204, max: 240)\n",
      "\n",
      "Run: 50, exploration: 0.01, score: 194\n",
      "Scores: (min: 16, avg: 110.4, max: 240)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Complete in 26\n",
      "Game Complete in 17\n",
      "Game Complete in 32\n",
      "Game Complete in 14\n",
      "Game Complete in 26\n",
      "Game Complete in 18\n",
      "Game Complete in 20\n",
      "Game Complete in 31\n",
      "Game Complete in 11\n",
      "Game Complete in 21\n",
      "Game Complete in 28\n",
      "Game Complete in 17\n",
      "Game Complete in 21\n",
      "Game Complete in 18\n",
      "Game Complete in 43\n",
      "Game Complete in 30\n",
      "Game Complete in 37\n",
      "Game Complete in 31\n",
      "Game Complete in 38\n",
      "Game Complete in 39\n",
      "Game Complete in 14\n",
      "Game Complete in 51\n",
      "Game Complete in 20\n",
      "Game Complete in 14\n",
      "Game Complete in 12\n",
      "Game Complete in 10\n",
      "Game Complete in 12\n",
      "Game Complete in 18\n",
      "Game Complete in 25\n",
      "Game Complete in 40\n",
      "RESULTS:  24.466666666666665\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def cartpoleload():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    dqn_solver.model = load_model(\"mymodel.h5\")\n",
    "    run = 0\n",
    "    res = 0\n",
    "    for i in range(0,30):\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "#             env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        print(f\"Game Complete in {step}\")\n",
    "        res = res + step\n",
    "    print(\"RESULTS: \", res/30)\n",
    "        \n",
    "\n",
    "cartpoleload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
